{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro To TP\n",
        "\n",
        "[Article Link](https://pytorch.org/tutorials/intermediate/TP_tutorial.html)\n",
        "\n",
        "[Dummy Training Link](https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py)\n",
        "\n",
        "It was introduced in the `Megatron-LM` paper. It is an efficient model parallelism technique to train large scale transformer models.\n",
        "\n",
        "SP (Sequence Parallel) is a variant of TP.\n",
        "- It shards on sequence dimension\n",
        "- for nn.LayerNorm or RMSNorm\n",
        "\n",
        "As the model size increases the activation memory becomes the bottleneck, as all the intermediate outputs has to be stored for gradient calculation.\n",
        "\n",
        "Therefore, in TP training SP is also applied to layers such as LayerNorm & RMSNorm"
      ],
      "metadata": {
        "id": "5b6ajNTtC-YL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How TP Works\n",
        "\n",
        "**Sharding Initialization**\n",
        "- Determine which `ParallelStyle` to apply to each layer and shard the initialized module by calling `parallelize_module`.\n",
        "- The parallelized modules would have their model parameters be swapped to DTensors, and DTensor would be responsible to run the parallelized module using sharded computation.\n",
        "\n",
        "**Runtime forward / backward**\n",
        "- Depending on the input/outputs DTensor layouts user specified for each `ParallelStyle`, it would run proper communication operation to transform the DTensor layouts for inputs/outputs (such as `allreduce`, `allgather`, `reduce_scatter`).\n",
        "- Run sharded computation for the parallelized layers to save compute/memory (e.g. `nn.Linear`, `nn.Embedding`)"
      ],
      "metadata": {
        "id": "4L-pZPBnFEmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When & Why TP should be applied\n",
        "\n",
        "PyTorch Fully Sharded Data Parallel (FSDP) already has the capability to scale model training to a specific number of GPUs. However, when it comes to further scale the model training in terms of model size and GPU quantity, many additional challenges arise that may require combining Tensor Parallel with FSDP.:\n",
        "\n",
        "- no of gpu >> 128/256 communication operations mentioned above are then dominated by `ring latency`.\n",
        "- Implementing TP/SP over FSDP helps to reduce the FSDP world size by 8 thereby reducing the latency.\n",
        "- `Hit data parallelism`, limit where global batch size cannot be greater than the world size due to convergence and GPU memory limitations.\n",
        "- TP is the only known way to help with this and allow scaling of both model & GPU.\n",
        "- For certain types of models (## TODO: IDENTIFY WHICH) when local batch size is smaller, TP/SP can yield matrix multiplication shapes that are more optimized for floating point operations (FLOPS)\n",
        "\n",
        "**Real World E.G**\n",
        "\n",
        "Llama 2 70B\n",
        "- GPUS - 2k\n",
        "- Global BS - 1k\n",
        "\n",
        "FSDP cannot alone work here at 2k GPU. Here local `batch size = 1` only is not possible due to convergence and memory constraints."
      ],
      "metadata": {
        "id": "KiKsQ4bwHLru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to apply Tensor Parallel\n",
        "\n",
        "Some module level primitives\n",
        "\n",
        "`ColwiseParallel` and `RowwiseParallel`: Shard the `nn.Linear` and `nn.Embedding` in the column or row fashion\n",
        "\n",
        "`SequenceParallel`: Perform sharded computations on the `nn.LayerNorm`, `nn.Dropout`, `RMSNormPython`\n",
        "\n",
        "`PrepareModuleInput` and `PrepareModuleOutput`: Configure the module inputs/outputs sharding layout with proper communication operations."
      ],
      "metadata": {
        "id": "9JpNJ2X7Sx3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s9wWvQtRM8R"
      },
      "outputs": [],
      "source": [
        "## TODO: Implement this using kaggle multi-gpu\n",
        "## Train on any random tensor"
      ]
    }
  ]
}